{"cells":[{"cell_type":"markdown","metadata":{"id":"3_vYh6wRUCZu"},"source":["# DETR for Tensorflow\n","\n","This notebook is a friendly tool for implementing my DETR object detection and multi-instance classification models on the COCO dataset.\n","\n","My models are coded in Tensorflow from first principles, as presented in the paper [End-to-End Object Detection with Transformers](https://ai.facebook.com/research/publications/end-to-end-object-detection-with-transformers) by Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AxBAMU9O5FGj"},"outputs":[],"source":["#\"\"\"\n","# automatically reload imports as they change (for debugging cusom imports)\n","%load_ext autoreload\n","%autoreload 2\n","#\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zd-_kRz3eFLJ"},"outputs":[],"source":["# Google Drive integration\n","# for model checkpointing (also for data loading if not using GCS)\n","from google.colab import drive\n","drive.mount('/content/drive')\n","#\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H43C1b-4EXCc"},"outputs":[],"source":["# Tensorflow\n","import tensorflow as tf\n","!pip install -q tensorflow-addons\n","import tensorflow_addons as tfa\n","tf.config.optimizer.set_jit(enabled=True)\n","!pip install -U tensorboard-plugin-profile\n","\n","# computation\n","import pandas as pd\n","import numpy as np\n","\n","# file system\n","import sys\n","import os\n","import glob\n","import shutil\n","import json\n","from zipfile import ZipFile\n","!pip install -q wget\n","\n","# Visualization\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","from matplotlib.patches import Rectangle\n","%matplotlib inline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r-spz6KMeomR"},"outputs":[],"source":["# custom imports\n","sys.path.insert(0, '/content/drive/MyDrive/GitHub/DETR_for_TF/ModelComponents')  # if using Google Drive\n","import boosted_DETR\n","import model\n","import model_pretrainer\n","import learning_rate_schedulers\n","import parameters\n","import datasets\n","import pipeline"]},{"cell_type":"markdown","metadata":{"id":"vJG5eGkH5dvU"},"source":["Load Modules"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a5TIZlDd5e3i"},"outputs":[],"source":["dataset_name = 'COCO'\n","model_parameters = parameters.ModelParameters(dataset_name=dataset_name)\n","params = model_parameters.default_params()\n","\n","\n","filepaths = parameters.Filepaths(dataset_name=dataset_name)\n","strategies = parameters.StrategyOptions(mixed_precision=True)\n","STRATEGY = strategies.strategy()"]},{"cell_type":"markdown","metadata":{"id":"dg03xGFofwOi"},"source":["## Load data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gk0B_GG1uqvb"},"outputs":[],"source":["coco = datasets.COCOStandard(local_base_dir='/content',\n","                             archive_base_dir='/content/drive/MyDrive/datasets/')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fud8izGSvmYj"},"outputs":[],"source":["coco.get_data(download=False, unzip=True, force_rebuild=False)"]},{"cell_type":"markdown","metadata":{"id":"5muipr06ftQS"},"source":["Prepare dataframes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"917YjZmwPolD"},"outputs":[],"source":["all_info_train = coco.prepare_COCO_from_json(subset='train', force_rebuild=False)\n","all_info_valid = coco.prepare_COCO_from_json(subset='val', force_rebuild=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aIaPaRLM3EUN"},"outputs":[],"source":["print('train samples:', len(all_info_train['annotations_df']))\n","print('valid samples:', len(all_info_valid['annotations_df']))"]},{"cell_type":"markdown","metadata":{"id":"q-WUsuRukz7U"},"source":["Create TF Datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jmi2A-zt5teM"},"outputs":[],"source":["data_pipeline = pipeline.Pipeline(**params)\n","image_augmentations = pipeline.Augmentations()"]},{"cell_type":"code","source":["all_info_train['annotations_df'].keys()"],"metadata":{"id":"IHNv81ec5cjk"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yyjHUlxWk1fP"},"outputs":[],"source":["ds_train = data_pipeline.data_generator(labels_df=all_info_train['annotations_df'],\n","                                        decode_images=True,\n","                                        stream_from_directory=False)\n","\n","ds_train_augmented = image_augmentations.apply_image_augmentations(ds_train)\n","\n","ds_valid = data_pipeline.data_generator(labels_df=all_info_valid['annotations_df'], \n","                                        decode_images=True,\n","                                        stream_from_directory=False)"]},{"cell_type":"markdown","metadata":{"id":"xk34jiUllXtO"},"source":["## Prepare Model"]},{"cell_type":"markdown","metadata":{"id":"ADQAPDZv6IsN"},"source":["Set Checkpoints"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4Pe1AhGglhJJ"},"outputs":[],"source":["CLASS_CHECKPOINT_DIR = os.path.join(filepaths.default_params('checkpoint_save_dir'), 'classification')\n","class_checkpoint_path = os.path.join(CLASS_CHECKPOINT_DIR, 'coco_class.ckpt')\n","\n","DETECTION_CHECKPOINT_DIR = os.path.join(filepaths.default_params('checkpoint_save_dir'), 'detection')\n","detection_checkpoint_path = os.path.join(DETECTION_CHECKPOINT_DIR, 'coco_detect.ckpt')\n","\n","\n","class_checkpoint = tf.keras.callbacks.ModelCheckpoint(class_checkpoint_path,\n","                                                      save_weights_only=True)\n","\n","detection_checkpoint = tf.keras.callbacks.ModelCheckpoint(detection_checkpoint_path,\n","                                                          save_weights_only=True)"]},{"cell_type":"markdown","metadata":{"id":"nLiGiFyXfpkD"},"source":["Examine Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lY7-DkPrfq5X"},"outputs":[],"source":["#\"\"\"\n","for val in ds_train_augmented.take(1):\n","    print(val.keys())\n","\n","ds_train.batch(1)\n","#\"\"\""]},{"cell_type":"markdown","metadata":{"id":"fHS2wtK1ftSR"},"source":["Box Visualization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lqr9gY8efuyP"},"outputs":[],"source":["def show_example(val, verbose=False):\n","\n","    image_id = val['image_id'].numpy()\n","    num_obj = val['num_objects'].numpy()\n","    image = val['image']\n","    bbox = val['bbox'][:num_obj, ...]\n","    category = val['category'][:num_obj, ...]\n","    attribute = val['attribute'][:num_obj, ...]\n","    orig_width = val['width'].numpy()\n","    orig_height = val['height'].numpy()\n","\n","\n","    if verbose:\n","        print('image_id:', image_id, 'num_obj:', num_obj)\n","        print('image:', image.shape, 'bbox:', bbox.shape, \n","              'category:', category.shape, 'attribute:', attribute.shape)\n","\n","    \n","    # display image\n","    fig = plt.figure()\n","    currentAxis = plt.gca()\n","    imgplot = plt.imshow(image, aspect=orig_height/orig_width)\n","\n","    # get image data\n","    image_height = image.shape[-3]\n","    image_width = image.shape[-2]\n","    boxes = bbox.numpy().tolist()\n","    categories = category.numpy().tolist()\n","    attributes = attribute.numpy().tolist()\n","\n","    # add boxes to image\n","    for i in range(len(boxes)):\n","        category = categories[i]\n","        \n","        # skip paddings\n","        if category == [b'<PAD>']:\n","            continue  \n","        attribute = attributes[i]\n","        box = boxes[i]\n","\n","        # report info\n","        print(f'box {i}', 'category:', category)\n","        print('attribute:', attribute)\n","\n","        # update box values for matplotlib\n","        xmin, ymin, xheight, yheight = box\n","        xmin = xmin * image_width\n","        width = xheight * image_width\n","        ymin = ymin * image_height\n","        height = yheight * image_height\n","\n","        currentAxis.add_patch(Rectangle((xmin, ymin),width, height,\n","                                alpha=1, fill=False, \n","                                label=category,\n","                                color = np.random.random(3)))\n","    fig.legend(loc='lower right', ncol=4)\n","    plt.show()\n","\n","    return plt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tzcZhCipfxUp"},"outputs":[],"source":["#\"\"\"\n","# training image\n","for val in ds_train.take(1):\n","    show_example(val, verbose=True)\n","#\"\"\""]},{"cell_type":"markdown","metadata":{"id":"H7eCUl7af3h1"},"source":["### Load Detection Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ov5VtwhkjJuP"},"outputs":[],"source":["LOAD_CLASS_WEIGHTS = False\n","# if False, models uses the most recent detection checkpoint weights\n","\n","\"\"\"  # Distributed computing code is commented out\n","with STRATEGY.scope():\n","\"\"\"\n","# DETECTION MODEL\n","# load base model and build\n","# attributes not provided in COCO dataset\n","detection_model = boosted_DETR.BoostedDETR(**params, attribute_weight=0.0)  \n","\n","# build\n","for val in ds_valid.batch(3).take(1):\n","    out_detect_0 = detection_model(val)\n","    out_detect_1 = detection_model(val, training=True)\n","\n","# learning rate\n","lr = tf.keras.optimizers.schedules.CosineDecayRestarts(\n","        initial_learning_rate=.001, first_decay_steps=4000, m_mul=.95, alpha=0.1)\n","\n","#optimizer_detect = tfa.optimizers.AdamW(learning_rate=.0001, weight_decay=.001, clipnorm=0.1)  # NOTE: suspect that using this with mixed precision causes NaNs from underflow/overflow\n","optimizer_detect = tf.keras.optimizers.SGD(learning_rate=lr, momentum=.9, \n","                                           nesterov=True, clipnorm=0.1)\n","\n","\n","# compile\n","detection_model.compile(optimizer=optimizer_detect)  # loss functions are built in\n","\n","# load weights\n","detect_checkpoint_filename = tf.train.latest_checkpoint(DETECTION_CHECKPOINT_DIR)\n","detection_model.load_weights(detect_checkpoint_filename)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zXoCT9Dpj8K-"},"outputs":[],"source":["# examine\n","# detection_model.summary()"]},{"cell_type":"markdown","metadata":{"id":"7UwYcQ_A8RXS"},"source":["# Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f_Jb0MFXMHiF"},"outputs":[],"source":["%load_ext tensorboard\n","%tensorboard --logdir logs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9IKRcUmwb4gr"},"outputs":[],"source":["## Train Detection Model\n","NUM_EPOCHS = 20\n","BATCH_SIZE = 8\n","\n","# lock layers\n","detection_model.EncoderBackbone.trainable = False\n","\n","#\"\"\"\n","train_block=1\n","for i in range(detection_model.num_decoder_blocks):\n","\n","    if i == train_block:\n","        train = True\n","    else:\n","        train = False\n","    detection_model.EncoderTransformerBlocks[i].trainable = train\n","    detection_model.DecoderBlocks[i].trainable = train\n","    detection_model.CategoryBlocks[i].trainable = train\n","    detection_model.AttributeBlocks[i].trainable = train\n","    detection_model.BoxBlocks[i].trainable = train\n","\n","optimizer_detect = tf.keras.optimizers.SGD(learning_rate=lr, momentum=.95, nesterov=True, clipnorm=0.1)\n","detection_model.compile(optimizer=optimizer_detect)\n","#\"\"\"\n","\n","ds = ds_train_augmented.batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n","#ds = ds_train.batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n","validation_data = ds_valid.batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE) \n","\n","detection_model.fit(ds, epochs=NUM_EPOCHS, \n","                    validation_data=validation_data,\n","                    callbacks=[detection_checkpoint, tf.keras.callbacks.TerminateOnNaN(),\n","                            tf.keras.callbacks.TensorBoard()],\n","                    )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PhpAf-l1RTN9","executionInfo":{"status":"aborted","timestamp":1639494834758,"user_tz":300,"elapsed":2,"user":{"displayName":"Mo Venouziou","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02927845103616843460"}}},"outputs":[],"source":["def show_prediction(val, model):\n","\n","    print('True Values:')\n","    print('image_id:', val['image_id'].shape)\n","    print('image:', val['image'].shape, 'bbox:', val['bbox'].shape, \n","          'category:', val['category'].shape, 'attribute:', val['attribute'].shape) \n","    \n","    category, attribute, box_coord_preds = model(val)\n","\n","    # update values\n","    val['image_id'] = val['image_id'][0, ...]\n","    val['image'] = val['image'][0, ...]\n","    val['bbox'] = box_coord_preds[0, ...]\n","    val['category'] = category[0, ...]\n","    val['attribute'] = attribute[0, ...]\n","    val['num_objects'] = tf.constant(200)\n","\n","    outs = show_example(val, verbose=True)\n","    return outs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b-uwwrt06U93","executionInfo":{"status":"aborted","timestamp":1639494834760,"user_tz":300,"elapsed":1,"user":{"displayName":"Mo Venouziou","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02927845103616843460"}}},"outputs":[],"source":["#\"\"\"\n","for val in ds_train.batch(1).take(1):\n","    show_prediction(val, detection_model)\n","#\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9LIXvQDHaw1X","executionInfo":{"status":"aborted","timestamp":1639494834762,"user_tz":300,"elapsed":2,"user":{"displayName":"Mo Venouziou","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02927845103616843460"}}},"outputs":[],"source":[""]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Boosted_DETR_COCO.ipynb","provenance":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}